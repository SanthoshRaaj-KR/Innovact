{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d72d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "# ---------------- Model Definition ----------------\n",
    "class DeepfakeDetectorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeepfakeDetectorCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# ---------------- Feature Extractor ----------------\n",
    "class AudioFeatureExtractor:\n",
    "    def __init__(self, sample_rate=22050, n_mels=128, max_len=128):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def extract_mel_spectrogram(self, file_path):\n",
    "        y, sr = librosa.load(file_path, sr=self.sample_rate, mono=True, duration=5.0)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels, hop_length=512)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        if mel_spec_db.shape[1] < self.max_len:\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, self.max_len - mel_spec_db.shape[1])), mode=\"constant\")\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :self.max_len]\n",
    "\n",
    "        return mel_spec_db.astype(np.float32)\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, extractor):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel_spec = self.extractor.extract_mel_spectrogram(self.file_paths[idx])\n",
    "        mel_spec = torch.tensor(mel_spec).unsqueeze(0)  # (1, n_mels, max_len)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return mel_spec, label\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, device, epochs=20):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate_model(val_loader, model, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={running_loss/len(train_loader):.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            torch.save(model.state_dict(), \"sound_deepfake_detector.pth\")\n",
    "            best_acc = val_acc\n",
    "            print(\"âœ… Model saved!\")\n",
    "\n",
    "def evaluate_model(loader, model, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_DIR = \"/fake_or_real_dataset\"  \n",
    "\n",
    "    real_files = [os.path.join(DATASET_DIR, \"real\", f) for f in os.listdir(os.path.join(DATASET_DIR, \"real\"))]\n",
    "    fake_files = [os.path.join(DATASET_DIR, \"fake\", f) for f in os.listdir(os.path.join(DATASET_DIR, \"fake\"))]\n",
    "\n",
    "    X = real_files + fake_files\n",
    "    y = [0] * len(real_files) + [1] * len(fake_files)\n",
    "\n",
    "    train_files, val_files, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    extractor = AudioFeatureExtractor()\n",
    "    train_dataset = AudioDataset(train_files, train_labels, extractor)\n",
    "    val_dataset = AudioDataset(val_files, val_labels, extractor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = DeepfakeDetectorCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_model(train_loader, val_loader, model, criterion, optimizer, device, epochs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
