{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4e4458-5df3-462e-9ed2-76e30c593bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection'...\n",
      "remote: Enumerating objects: 162, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 162 (delta 28), reused 14 (delta 14), pack-reused 123 (from 1)\u001b[K\n",
      "Receiving objects: 100% (162/162), 1.10 MiB | 2.64 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "/workspace/Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/davide-coccomini/Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection.git\n",
    "%cd Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96eabc2b-9d99-4ad1-9bc5-cca1ccb579ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7ae098abdc4e0fb4b5656263dbe7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
    "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
    "import kagglehub\n",
    "kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539e67eb-9bdd-47a0-b56c-8661c06e5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/binshilin/final-dataset-genreal?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.26G/8.26G [01:58<00:00, 75.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "binshilin_genreal_old_deepfakes_path = kagglehub.dataset_download('binshilin/final-dataset-genreal')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4838467a-1e66-45b9-8b14-7c7c278778ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow==9.5.0\n",
      "  Downloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Downloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "Successfully installed Pillow-9.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet_pytorch) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch->efficientnet_pytorch) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->efficientnet_pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16478 sha256=e024bc6f2f3d2080335b7d58bce30719df1bd08ce70261a5124fbe49031a7bdf\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.1.2)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.12.0.88\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow==9.5.0 --force-reinstall\n",
    "!pip install efficientnet_pytorch tqdm\n",
    "!pip install opencv-python\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb44b5-4d68-49b9-a1e7-048bf6a1a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Root dataset directory\n",
    "dataset_root = '/path/to/data_split'\n",
    "\n",
    "# Threshold for minimum number of frames\n",
    "MIN_FRAMES = 16\n",
    "\n",
    "# Function to remove subfolders with fewer than MIN_FRAMES images\n",
    "def clean_short_sequences(root_path):\n",
    "    removed_count = 0\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        # Only go one level deep from each person folder\n",
    "        if 'person_' in os.path.basename(root):\n",
    "            image_files = [f for f in os.listdir(root) if f.endswith(('.jpg', '.png'))]\n",
    "            if len(image_files) < MIN_FRAMES:\n",
    "                print(f\"Removing {root} ({len(image_files)} frames)\")\n",
    "                shutil.rmtree(root)\n",
    "                removed_count += 1\n",
    "    print(f\"✅ Removed {removed_count} folders with < {MIN_FRAMES} frames under {root_path}\\n\")\n",
    "\n",
    "# Apply it to all four paths\n",
    "for split in ['train', 'test']:\n",
    "    for label in ['real', 'fake']:\n",
    "        path = os.path.join(dataset_root, split, label)\n",
    "        clean_short_sequences(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL 1\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------- Dataset Class with Sequential Batching -----------\n",
    "class FaceSequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_frames=32, min_frames=16, frame_interval=1):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {'original': 0, 'fake': 1}\n",
    "        self.max_frames = max_frames\n",
    "        self.min_frames = min_frames\n",
    "        self.frame_interval = frame_interval\n",
    "\n",
    "        for label in ['original', 'fake']:\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            for folder in os.listdir(class_dir):\n",
    "                folder_path = os.path.join(class_dir, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                for person_folder in os.listdir(folder_path):\n",
    "                    person_path = os.path.join(folder_path, person_folder)\n",
    "                    if not os.path.isdir(person_path):\n",
    "                        continue\n",
    "                    frame_paths = sorted(glob.glob(os.path.join(person_path, '*.jpg')))\n",
    "                    \n",
    "                    # Create sequential batches from the frame sequence\n",
    "                    if len(frame_paths) >= self.min_frames:\n",
    "                        self._create_sequential_batches(frame_paths, self.label_map[label])\n",
    "\n",
    "    def _create_sequential_batches(self, frame_paths, label):\n",
    "        \"\"\"Create sequential batches of frames from a single video sequence\"\"\"\n",
    "        total_frames = len(frame_paths)\n",
    "        start_idx = 0\n",
    "        \n",
    "        while start_idx < total_frames:\n",
    "            end_idx = min(start_idx + self.max_frames, total_frames)\n",
    "            batch_frames = frame_paths[start_idx:end_idx]\n",
    "            \n",
    "            # Check if this batch meets our criteria\n",
    "            if len(batch_frames) >= self.min_frames:\n",
    "                self.samples.append((batch_frames, label))\n",
    "            # If less than min_frames, discard this batch\n",
    "            \n",
    "            start_idx += self.max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "        \n",
    "        images = []\n",
    "        for frame_path in frame_paths:\n",
    "            try:\n",
    "                img = Image.open(frame_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {frame_path}: {e}\")\n",
    "                # Use a black image as fallback\n",
    "                if self.transform:\n",
    "                    img = self.transform(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "                else:\n",
    "                    img = torch.zeros(3, 224, 224)\n",
    "                images.append(img)\n",
    "        \n",
    "        # Pad sequence if needed (for sequences with min_frames <= length < max_frames)\n",
    "        while len(images) < self.max_frames:\n",
    "            images.append(torch.zeros_like(images[0]))\n",
    "            \n",
    "        images = torch.stack(images)  # (T, C, H, W)\n",
    "        return images, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ----------- Improved Model with Proper Spatial-Temporal Processing -----------\n",
    "class SpatialTemporalDeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, max_frames=32, feature_dim=1408):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spatial feature extractor\n",
    "        self.spatial_encoder = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "        self.spatial_encoder._fc = nn.Identity()\n",
    "        \n",
    "        # Spatial feature projection\n",
    "        self.spatial_projection = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Temporal modeling with LSTM + Transformer\n",
    "        self.temporal_lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=512,  # bidirectional LSTM output\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Positional encoding for temporal sequences\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(max_frames, 512))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        \n",
    "        # Extract spatial features for each frame\n",
    "        x_reshaped = x.view(B * T, C, H, W)\n",
    "        spatial_features = self.spatial_encoder.extract_features(x_reshaped)  # (B*T, 1408, h, w)\n",
    "        \n",
    "        # Project spatial features\n",
    "        spatial_features = self.spatial_projection(spatial_features)  # (B*T, 512)\n",
    "        spatial_features = spatial_features.view(B, T, -1)  # (B, T, 512)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        spatial_features = spatial_features + self.pos_encoding[:T].unsqueeze(0)\n",
    "        \n",
    "        # Temporal modeling with LSTM\n",
    "        lstm_out, _ = self.temporal_lstm(spatial_features)  # (B, T, 512)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attended_features, attention_weights = self.temporal_attention(\n",
    "            lstm_out, lstm_out, lstm_out\n",
    "        )  # (B, T, 512)\n",
    "        \n",
    "        # Global temporal pooling\n",
    "        temporal_representation = torch.mean(attended_features, dim=1)  # (B, 512)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(temporal_representation)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ----------- Training Utilities -----------\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def analyze_dataset_composition(dataset):\n",
    "    \"\"\"Analyze how many samples were created from sequences of different lengths\"\"\"\n",
    "    print(f\"Total training samples created: {len(dataset)}\")\n",
    "    \n",
    "    # Count class distribution\n",
    "    original_count = sum(1 for _, label in dataset.samples if label == 0)\n",
    "    fake_count = sum(1 for _, label in dataset.samples if label == 1)\n",
    "    \n",
    "    print(f\"Original samples: {original_count}\")\n",
    "    print(f\"Fake samples: {fake_count}\")\n",
    "    print(f\"Class ratio (fake/original): {fake_count/original_count:.2f}\")\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=30, checkpoint_path='best_model.pth'):\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5]).to(device))  # Weight for class imbalance\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    early_stop_counter = 0\n",
    "    patience = 7\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        train_acc, train_prec, train_rec, train_f1 = calculate_metrics(all_labels, all_preds)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc, val_prec, val_rec, val_f1 = calculate_metrics(all_labels, all_preds)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Val - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "        # Save best model based on F1 score\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"New best model saved with F1: {val_f1:.4f}\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                break\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Training completed. Best validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# ----------- Main Execution -----------\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    train_dir = '/root/.cache/kagglehub/datasets/binshilin/final-dataset-genreal/versions/1/train'\n",
    "    test_dir = '/root/.cache/kagglehub/datasets/binshilin/final-dataset-genreal/versions/1/test'\n",
    "\n",
    "    # Enhanced data augmentation for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
    "        ], p=0.7),\n",
    "        transforms.RandomGrayscale(p=0.05),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomApply([\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5))\n",
    "        ], p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ])\n",
    "\n",
    "    # Simple transform for validation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create datasets with sequential batching\n",
    "    print(\"Creating training dataset with sequential batching...\")\n",
    "    train_dataset = FaceSequenceDataset(train_dir, transform=train_transform, max_frames=32, min_frames=16)\n",
    "    \n",
    "    print(\"Creating validation dataset with sequential batching...\")\n",
    "    val_dataset = FaceSequenceDataset(test_dir, transform=val_transform, max_frames=32, min_frames=16)\n",
    "\n",
    "    # Analyze dataset composition\n",
    "    print(\"\\n=== Training Dataset Analysis ===\")\n",
    "    analyze_dataset_composition(train_dataset)\n",
    "    print(\"\\n=== Validation Dataset Analysis ===\")\n",
    "    analyze_dataset_composition(val_dataset)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=2,  # Adjust based on your GPU memory\n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=2, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model and training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    model = SpatialTemporalDeepfakeDetector().to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    print(f\"\\nDataLoader info:\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "    # Start training\n",
    "    print(\"\\nStarting training...\")\n",
    "    train(model, train_loader, val_loader, device, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013718f-dd90-420e-9a5c-b30b1fe61bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinSpace for frames (Not what we want)\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------- Dataset Class -----------\n",
    "class FaceSequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_frames=32, frame_interval=1):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {'original': 0, 'fake': 1}\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_interval = frame_interval\n",
    "\n",
    "        for label in ['original', 'fake']:\n",
    "            class_dir = os.path.join(root_dir, label)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "            for folder in os.listdir(class_dir):\n",
    "                folder_path = os.path.join(class_dir, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                for person_folder in os.listdir(folder_path):\n",
    "                    person_path = os.path.join(folder_path, person_folder)\n",
    "                    if not os.path.isdir(person_path):\n",
    "                        continue\n",
    "                    frame_paths = sorted(glob.glob(os.path.join(person_path, '*.jpg')))\n",
    "                    if len(frame_paths) >= self.max_frames:\n",
    "                        self.samples.append((frame_paths, self.label_map[label]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "        \n",
    "        # Sample frames uniformly to maintain temporal order\n",
    "        if len(frame_paths) > self.max_frames:\n",
    "            # Sample uniformly across the sequence\n",
    "            indices = np.linspace(0, len(frame_paths)-1, self.max_frames, dtype=int)\n",
    "            frame_paths = [frame_paths[i] for i in indices]\n",
    "        \n",
    "        images = []\n",
    "        for frame_path in frame_paths:\n",
    "            try:\n",
    "                img = Image.open(frame_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {frame_path}: {e}\")\n",
    "                # Use a black image as fallback\n",
    "                img = torch.zeros(3, 224, 224)\n",
    "                images.append(img)\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(images) < self.max_frames:\n",
    "            images.append(torch.zeros_like(images[0]))\n",
    "            \n",
    "        images = torch.stack(images)  # (T, C, H, W)\n",
    "        return images, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# ----------- Improved Model with Proper Spatial-Temporal Processing -----------\n",
    "class SpatialTemporalDeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, max_frames=32, feature_dim=1408):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Spatial feature extractor\n",
    "        self.spatial_encoder = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "        self.spatial_encoder._fc = nn.Identity()\n",
    "        \n",
    "        # Spatial feature projection\n",
    "        self.spatial_projection = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Temporal modeling with LSTM + Transformer\n",
    "        self.temporal_lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=512,  # bidirectional LSTM output\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Positional encoding for temporal sequences\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(max_frames, 512))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        \n",
    "        # Extract spatial features for each frame\n",
    "        x_reshaped = x.view(B * T, C, H, W)\n",
    "        spatial_features = self.spatial_encoder.extract_features(x_reshaped)  # (B*T, 1408, h, w)\n",
    "        \n",
    "        # Project spatial features\n",
    "        spatial_features = self.spatial_projection(spatial_features)  # (B*T, 512)\n",
    "        spatial_features = spatial_features.view(B, T, -1)  # (B, T, 512)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        spatial_features = spatial_features + self.pos_encoding[:T].unsqueeze(0)\n",
    "        \n",
    "        # Temporal modeling with LSTM\n",
    "        lstm_out, _ = self.temporal_lstm(spatial_features)  # (B, T, 512)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attended_features, attention_weights = self.temporal_attention(\n",
    "            lstm_out, lstm_out, lstm_out\n",
    "        )  # (B, T, 512)\n",
    "        \n",
    "        # Global temporal pooling\n",
    "        temporal_representation = torch.mean(attended_features, dim=1)  # (B, 512)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(temporal_representation)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ----------- Training Utilities -----------\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=30, checkpoint_path='best_model.pth'):\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5]).to(device))  # Weight for class imbalance\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    early_stop_counter = 0\n",
    "    patience = 7\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if batch_idx % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        train_acc, train_prec, train_rec, train_f1 = calculate_metrics(all_labels, all_preds)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, all_preds, all_labels = 0, [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc, val_prec, val_rec, val_f1 = calculate_metrics(all_labels, all_preds)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Val - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_loss': avg_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "        \n",
    "        # Save best model based on F1 score\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"New best model saved with F1: {val_f1:.4f}\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                break\n",
    "        \n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Training completed. Best validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# ----------- Setup -----------\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    train_dir = '/root/.cache/kagglehub/datasets/binshilin/final-dataset-genreal/versions/1/train'\n",
    "    test_dir = '/root/.cache/kagglehub/datasets/binshilin/final-dataset-genreal/versions/1/test'\n",
    "\n",
    "    # Enhanced data augmentation for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
    "        ], p=0.7),\n",
    "        transforms.RandomGrayscale(p=0.05),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomApply([\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5))\n",
    "        ], p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ])\n",
    "\n",
    "    # Simple transform for validation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = FaceSequenceDataset(train_dir, transform=train_transform, max_frames=32)\n",
    "    val_dataset = FaceSequenceDataset(test_dir, transform=val_transform, max_frames=32)\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=2,  # Increased batch size\n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=2, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model and training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = SpatialTemporalDeepfakeDetector().to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Start training\n",
    "    train(model, train_loader, val_loader, device, num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
