{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "e23d468e2cb14a73a97fa77af2bd175f",
            "a3235b114c564c6d9ead4a983a346e4b",
            "29026a3d238a4487b50e6f50b7dbe466",
            "fce0bc7a6eba4081a3695e4fabc2174d",
            "31e01af87ffe4ec4b3eccc104868a252",
            "6f8bd986bc9f4d148ee753c3339c51f7",
            "fa45806238ea494eb9f33f8c2e7f467f",
            "b9562536a93a415e9bd3778c427fe940",
            "bff307168ce74a63a2bd8be5309d642d",
            "3b2c625b67fe4acb913cd0eea3c1b3ab",
            "283c175e4029448883b74730b0c4cac3",
            "06a38ef815cc44a996de1ff144421474",
            "090a19958ad849dda60e05f4a3499132",
            "b8904ddf76894d83b950dbe58bd4e57e",
            "12a604cf454d49faa3a70d4b74f1d382",
            "669acd1f0a9f4b9d8602440cbba8a277",
            "941a0abcfeec42efaa006b23c8a20838",
            "1c6486e04243460bbc462141ce0cf834",
            "2fd7f319c8d44cadb0e3cd573f0b6578",
            "71bacabea97744029240057928a4d958",
            "b7608b73efca4de6b3025d7d20a70173",
            "543c9b42a79e44bca577fd9ed81fac14",
            "aceeeb4144bd4d9fa213fdb4590192a7"
          ]
        },
        "id": "QVqZgGhOXrHf",
        "outputId": "1cf4cc0a-bff5-495c-978d-bc6ad460f37b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e23d468e2cb14a73a97fa77af2bd175f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "\n",
        "# Now login\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTlQGV-XZC8a",
        "outputId": "c3c1d0d0-e24d-431b-a471-824e06c3b172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/binshilin/poc-final-dataset-vv?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 155M/155M [00:05<00:00, 30.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n",
            "Dataset path: /root/.cache/kagglehub/datasets/binshilin/poc-final-dataset-vv/versions/1\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import kagglehub\n",
        "\n",
        "# Login is assumed to be done already\n",
        "binshilin_genreal_old_deepfakes_path = kagglehub.dataset_download('binshilin/poc-final-dataset-vv')\n",
        "\n",
        "print('Data source import complete.')\n",
        "dataset_path = Path(binshilin_genreal_old_deepfakes_path)\n",
        "print(f'Dataset path: {str(dataset_path)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OADIHZ5CoPsB",
        "outputId": "a4e2a21a-2bcf-4071-d8bc-88464e799185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from efficientnet_pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->efficientnet_pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=c7807df314e87db4be43899bcfeedb2100f871a077b90251d95700e284b34f1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/3f/43/e6271c7026fe08c185da2be23c98c8e87477d3db63f41f32ad\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ],
      "source": [
        "! pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NRl0hhzhaJm0",
        "outputId": "7caa6985-7085-4a5d-8ba7-6387254f674e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating training dataset with sequential batching...\n",
            "Scanning dataset in: /root/.cache/kagglehub/datasets/binshilin/poc-final-dataset-vv/versions/1/train\n",
            "Using max_frames=32, min_frames=16\n",
            "Processing original class...\n",
            "Skipping clip_tBhx3g7hXNw_person_001: only 3 frames (< 16)\n",
            "Skipping clip_lcClO5lHEjA_person_002: only 1 frames (< 16)\n",
            "Processed 100 original folders, created 204 total batches so far\n",
            "Finished processing original class: 100 folders\n",
            "Processing fake class...\n",
            "Skipping Tina_Shift_0_right_person_002: only 1 frames (< 16)\n",
            "Skipping Eyelevel_closeup__202508101740_ez9gb_person_003: only 1 frames (< 16)\n",
            "Processed 100 fake folders, created 484 total batches so far\n",
            "Finished processing fake class: 100 folders\n",
            "Creating validation dataset with sequential batching...\n",
            "Scanning dataset in: /root/.cache/kagglehub/datasets/binshilin/poc-final-dataset-vv/versions/1/test\n",
            "Using max_frames=32, min_frames=16\n",
            "Processing original class...\n",
            "Skipping clip_z0gv5NQPuv4_person_001: only 1 frames (< 16)\n",
            "Finished processing original class: 10 folders\n",
            "Processing fake class...\n",
            "Finished processing fake class: 8 folders\n",
            "\n",
            "=== Training Dataset Analysis ===\n",
            "Total samples created: 484\n",
            "Original samples: 204\n",
            "Fake samples: 280\n",
            "Class ratio (fake/original): 1.37\n",
            "Batch length distribution:\n",
            "  Min: 16\n",
            "  Max: 32\n",
            "  Mean: 30.7\n",
            "Batches with exactly 32 frames: 401 (82.9%)\n",
            "\n",
            "=== Validation Dataset Analysis ===\n",
            "Total samples created: 45\n",
            "Original samples: 20\n",
            "Fake samples: 25\n",
            "Class ratio (fake/original): 1.25\n",
            "Batch length distribution:\n",
            "  Min: 18\n",
            "  Max: 32\n",
            "  Mean: 30.6\n",
            "Batches with exactly 32 frames: 37 (82.2%)\n",
            "\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n",
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b2-8bb594d6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 35.1M/35.1M [00:00<00:00, 67.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b2\n",
            "Total parameters: 12,807,812\n",
            "Trainable parameters: 12,807,812\n",
            "\n",
            "DataLoader info:\n",
            "Train batches: 242\n",
            "Validation batches: 23\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - Training: 100%|██████████| 242/242 [02:32<00:00,  1.59it/s, loss=0.0480]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5717, Acc: 0.6715, F1: 0.7602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 23/23 [00:08<00:00,  2.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val - Loss: 0.3932, Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "------------------------------------------------------------\n",
            "New best model saved with F1: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/30 - Training: 100%|██████████| 242/242 [02:08<00:00,  1.89it/s, loss=0.4835]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.3380, Acc: 0.8574, F1: 0.8774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 23/23 [00:04<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val - Loss: 0.1850, Acc: 1.0000, Prec: 1.0000, Rec: 1.0000, F1: 1.0000\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/30 - Training:   9%|▉         | 22/242 [00:14<02:22,  1.55it/s, loss=0.0852]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-465345344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-465345344.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training with flattened dataset (Person_001, Person_002 all have come outside)\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ----------- Dataset Class with Sequential Batching -----------\n",
        "class FaceSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_frames=32, min_frames=16, frame_interval=1):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.label_map = {'original': 0, 'fake': 1}\n",
        "        self.max_frames = max_frames\n",
        "        self.min_frames = min_frames\n",
        "        self.frame_interval = frame_interval\n",
        "\n",
        "        print(f\"Scanning dataset in: {root_dir}\")\n",
        "        print(f\"Using max_frames={max_frames}, min_frames={min_frames}\")\n",
        "\n",
        "        for label in ['original', 'fake']:\n",
        "            class_dir = os.path.join(root_dir, label)\n",
        "            if not os.path.exists(class_dir):\n",
        "                print(f\"Warning: {class_dir} does not exist, skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing {label} class...\")\n",
        "            folder_count = 0\n",
        "\n",
        "            # Iterate through video folders directly under original/fake\n",
        "            for folder_name in os.listdir(class_dir):\n",
        "                folder_path = os.path.join(class_dir, folder_name)\n",
        "                if not os.path.isdir(folder_path):\n",
        "                    continue\n",
        "\n",
        "                folder_count += 1\n",
        "\n",
        "                # Get all frame files in this video folder\n",
        "                frame_paths = []\n",
        "                for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "                    frame_paths.extend(glob.glob(os.path.join(folder_path, ext)))\n",
        "\n",
        "                # Sort frames by filename to maintain temporal order\n",
        "                frame_paths = sorted(frame_paths)\n",
        "\n",
        "                if len(frame_paths) == 0:\n",
        "                    print(f\"Warning: No frames found in {folder_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Create sequential batches from this video's frames\n",
        "                batches_created = self._create_sequential_batches(frame_paths, self.label_map[label], folder_name)\n",
        "\n",
        "                if folder_count % 100 == 0:\n",
        "                    print(f\"Processed {folder_count} {label} folders, created {len(self.samples)} total batches so far\")\n",
        "\n",
        "            print(f\"Finished processing {label} class: {folder_count} folders\")\n",
        "\n",
        "    def _create_sequential_batches(self, frame_paths, label, folder_name):\n",
        "        \"\"\"Create sequential batches of frames from a single video sequence\"\"\"\n",
        "        total_frames = len(frame_paths)\n",
        "        batches_created = 0\n",
        "\n",
        "        if total_frames < self.min_frames:\n",
        "            print(f\"Skipping {folder_name}: only {total_frames} frames (< {self.min_frames})\")\n",
        "            return batches_created\n",
        "\n",
        "        # Create overlapping or non-overlapping batches\n",
        "        # Using stride = max_frames for non-overlapping batches\n",
        "        # You can change this to max_frames//2 for overlapping batches\n",
        "        stride = self.max_frames  # Non-overlapping batches\n",
        "\n",
        "        start_idx = 0\n",
        "        while start_idx < total_frames:\n",
        "            end_idx = min(start_idx + self.max_frames, total_frames)\n",
        "            batch_frames = frame_paths[start_idx:end_idx]\n",
        "\n",
        "            # Only keep batches that meet minimum frame requirement\n",
        "            if len(batch_frames) >= self.min_frames:\n",
        "                self.samples.append((batch_frames, label, folder_name))\n",
        "                batches_created += 1\n",
        "\n",
        "            start_idx += stride\n",
        "\n",
        "            # If remaining frames are less than min_frames, break\n",
        "            if total_frames - start_idx < self.min_frames:\n",
        "                break\n",
        "\n",
        "        return batches_created\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_paths, label, folder_name = self.samples[idx]\n",
        "\n",
        "        images = []\n",
        "        successful_loads = 0\n",
        "\n",
        "        for frame_path in frame_paths:\n",
        "            try:\n",
        "                img = Image.open(frame_path).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                images.append(img)\n",
        "                successful_loads += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {frame_path}: {e}\")\n",
        "                # Use a black image as fallback\n",
        "                if self.transform:\n",
        "                    fallback_img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "                    img = self.transform(fallback_img)\n",
        "                else:\n",
        "                    img = torch.zeros(3, 224, 224)\n",
        "                images.append(img)\n",
        "\n",
        "        # Pad sequence to max_frames if needed\n",
        "        while len(images) < self.max_frames:\n",
        "            # Duplicate the last frame for padding\n",
        "            if len(images) > 0:\n",
        "                images.append(images[-1].clone())\n",
        "            else:\n",
        "                # Fallback if no images were loaded successfully\n",
        "                if self.transform:\n",
        "                    fallback_img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "                    img = self.transform(fallback_img)\n",
        "                else:\n",
        "                    img = torch.zeros(3, 224, 224)\n",
        "                images.append(img)\n",
        "\n",
        "        images = torch.stack(images)  # (T, C, H, W)\n",
        "\n",
        "        # Create a mask for valid frames (non-padded)\n",
        "        valid_frames = min(len(frame_paths), self.max_frames)\n",
        "\n",
        "        return images, torch.tensor(label, dtype=torch.long), valid_frames\n",
        "\n",
        "# ----------- Improved Model with Proper Spatial-Temporal Processing -----------\n",
        "class SpatialTemporalDeepfakeDetector(nn.Module):\n",
        "    def __init__(self, num_classes=2, max_frames=32, feature_dim=1408):\n",
        "        super().__init__()\n",
        "\n",
        "        # Spatial feature extractor\n",
        "        self.spatial_encoder = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        self.spatial_encoder._fc = nn.Identity()\n",
        "\n",
        "        # Spatial feature projection\n",
        "        self.spatial_projection = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Temporal modeling with LSTM + Transformer\n",
        "        self.temporal_lstm = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Temporal attention\n",
        "        self.temporal_attention = nn.MultiheadAttention(\n",
        "            embed_dim=512,  # bidirectional LSTM output\n",
        "            num_heads=8,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Positional encoding for temporal sequences\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(max_frames, 512))\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, valid_frames=None):  # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Extract spatial features for each frame\n",
        "        x_reshaped = x.view(B * T, C, H, W)\n",
        "        spatial_features = self.spatial_encoder.extract_features(x_reshaped)  # (B*T, 1408, h, w)\n",
        "\n",
        "        # Project spatial features\n",
        "        spatial_features = self.spatial_projection(spatial_features)  # (B*T, 512)\n",
        "        spatial_features = spatial_features.view(B, T, -1)  # (B, T, 512)\n",
        "\n",
        "        # Add positional encoding\n",
        "        spatial_features = spatial_features + self.pos_encoding[:T].unsqueeze(0)\n",
        "\n",
        "        # Temporal modeling with LSTM\n",
        "        lstm_out, _ = self.temporal_lstm(spatial_features)  # (B, T, 512)\n",
        "\n",
        "        # Create attention mask for padded frames (optional)\n",
        "        if valid_frames is not None:\n",
        "            # Create mask for attention\n",
        "            mask = torch.zeros(B, T, dtype=torch.bool, device=x.device)\n",
        "            for i, vf in enumerate(valid_frames):\n",
        "                mask[i, :vf] = True\n",
        "            attn_mask = ~mask\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        # Temporal attention\n",
        "        attended_features, attention_weights = self.temporal_attention(\n",
        "            lstm_out, lstm_out, lstm_out, key_padding_mask=attn_mask\n",
        "        )  # (B, T, 512)\n",
        "\n",
        "        # Global temporal pooling (weighted by valid frames)\n",
        "        if valid_frames is not None:\n",
        "            # Weighted average based on valid frames\n",
        "            temporal_representation = []\n",
        "            for i, vf in enumerate(valid_frames):\n",
        "                temporal_representation.append(\n",
        "                    torch.mean(attended_features[i, :vf], dim=0)\n",
        "                )\n",
        "            temporal_representation = torch.stack(temporal_representation)\n",
        "        else:\n",
        "            temporal_representation = torch.mean(attended_features, dim=1)  # (B, 512)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(temporal_representation)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ----------- Training Utilities -----------\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def analyze_dataset_composition(dataset):\n",
        "    \"\"\"Analyze how many samples were created from sequences of different lengths\"\"\"\n",
        "    print(f\"Total samples created: {len(dataset)}\")\n",
        "\n",
        "    # Count class distribution\n",
        "    original_count = sum(1 for _, label, _ in dataset.samples if label == 0)\n",
        "    fake_count = sum(1 for _, label, _ in dataset.samples if label == 1)\n",
        "\n",
        "    print(f\"Original samples: {original_count}\")\n",
        "    print(f\"Fake samples: {fake_count}\")\n",
        "    print(f\"Class ratio (fake/original): {fake_count/max(original_count, 1):.2f}\")\n",
        "\n",
        "    # Analyze batch sizes\n",
        "    batch_lengths = [len(frames) for frames, _, _ in dataset.samples]\n",
        "    print(f\"Batch length distribution:\")\n",
        "    print(f\"  Min: {min(batch_lengths)}\")\n",
        "    print(f\"  Max: {max(batch_lengths)}\")\n",
        "    print(f\"  Mean: {np.mean(batch_lengths):.1f}\")\n",
        "\n",
        "    # Count how many batches are exactly max_frames\n",
        "    full_batches = sum(1 for length in batch_lengths if length == dataset.max_frames)\n",
        "    print(f\"Batches with exactly {dataset.max_frames} frames: {full_batches} ({full_batches/len(batch_lengths)*100:.1f}%)\")\n",
        "\n",
        "def train(model, train_loader, val_loader, device, num_epochs=30, checkpoint_path='best_model.pth'):\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.5]).to(device))  # Weight for class imbalance\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    early_stop_counter = 0\n",
        "    patience = 7\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, all_preds, all_labels = 0, [], []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "        for batch_idx, batch_data in enumerate(progress_bar):\n",
        "            if len(batch_data) == 3:\n",
        "                inputs, labels, valid_frames = batch_data\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "                valid_frames = valid_frames.to(device, non_blocking=True)\n",
        "            else:\n",
        "                inputs, labels = batch_data\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "                valid_frames = None\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs, valid_frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            # Clear cache periodically\n",
        "            if batch_idx % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        train_acc, train_prec, train_rec, train_f1 = calculate_metrics(all_labels, all_preds)\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        print(f\"Train - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, all_preds, all_labels = 0, [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_data in tqdm(val_loader, desc=\"Validating\"):\n",
        "                if len(batch_data) == 3:\n",
        "                    inputs, labels, valid_frames = batch_data\n",
        "                    inputs = inputs.to(device, non_blocking=True)\n",
        "                    labels = labels.to(device, non_blocking=True)\n",
        "                    valid_frames = valid_frames.to(device, non_blocking=True)\n",
        "                else:\n",
        "                    inputs, labels = batch_data\n",
        "                    inputs = inputs.to(device, non_blocking=True)\n",
        "                    labels = labels.to(device, non_blocking=True)\n",
        "                    valid_frames = None\n",
        "\n",
        "                with autocast():\n",
        "                    outputs = model(inputs, valid_frames)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc, val_prec, val_rec, val_f1 = calculate_metrics(all_labels, all_labels)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Val - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_f1': val_f1,\n",
        "            'val_loss': avg_val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "\n",
        "        # Save best model based on F1 score\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"New best model saved with F1: {val_f1:.4f}\")\n",
        "            early_stop_counter = 0\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
        "                break\n",
        "\n",
        "        # Clear cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Training completed. Best validation F1: {best_val_f1:.4f}\")\n",
        "\n",
        "# ----------- Main Execution -----------\n",
        "if __name__ == '__main__':\n",
        "    # Paths - Update these to match your dataset structure\n",
        "    train_dir = '/root/.cache/kagglehub/datasets/binshilin/poc-final-dataset-vv/versions/1/train'\n",
        "    test_dir = '/root/.cache/kagglehub/datasets/binshilin/poc-final-dataset-vv/versions/1/test'\n",
        "\n",
        "    # Enhanced data augmentation for training\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
        "        ], p=0.7),\n",
        "        transforms.RandomGrayscale(p=0.05),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomApply([\n",
        "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5))\n",
        "        ], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Simple transform for validation\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets with sequential batching\n",
        "    print(\"Creating training dataset with sequential batching...\")\n",
        "    train_dataset = FaceSequenceDataset(train_dir, transform=train_transform, max_frames=32, min_frames=16)\n",
        "\n",
        "    print(\"Creating validation dataset with sequential batching...\")\n",
        "    val_dataset = FaceSequenceDataset(test_dir, transform=val_transform, max_frames=32, min_frames=16)\n",
        "\n",
        "    # Analyze dataset composition\n",
        "    print(\"\\n=== Training Dataset Analysis ===\")\n",
        "    analyze_dataset_composition(train_dataset)\n",
        "    print(\"\\n=== Validation Dataset Analysis ===\")\n",
        "    analyze_dataset_composition(val_dataset)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=2,  # Adjust based on your GPU memory\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=2,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model and training\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    model = SpatialTemporalDeepfakeDetector().to(device)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    print(f\"\\nDataLoader info:\")\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    # Start training\n",
        "    print(\"\\nStarting training...\")\n",
        "    train(model, train_loader, val_loader, device, num_epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3KrMbxVZdFq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpOY2JOVZc8l"
      },
      "outputs": [],
      "source": [
        "# Inference batch wise split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2SyQfd7CO65",
        "outputId": "cd1ec40e-0b25-4cf5-84fb-e3365dca2fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.12)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting keras-facenet\n",
            "  Downloading keras-facenet-0.3.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.1)\n",
            "Collecting lz4>=4.3.3 (from mtcnn)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.8.3)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->facenet-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision->facenet-pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->facenet-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision->facenet-pytorch) (3.0.2)\n",
            "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: keras-facenet\n",
            "  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-py3-none-any.whl size=10367 sha256=effd08ab9d7c9d5c56a5ca2f18abe564002ac947cbaf1d19d6034d1dcfa42bd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/b0/f5/19ac49fedc10b1df3ee56b096edbcfa39d45794fccc6bcdbbf\n",
            "Successfully built keras-facenet\n",
            "Installing collected packages: lz4, mtcnn, deep_sort_realtime, keras-facenet, facenet-pytorch\n",
            "Successfully installed deep_sort_realtime-1.3.2 facenet-pytorch-2.5.3 keras-facenet-0.3.2 lz4-4.4.4 mtcnn-1.0.0\n"
          ]
        }
      ],
      "source": [
        "! pip install kagglehub opencv-python mtcnn deep_sort_realtime opencv-python mtcnn keras-facenet numpy scipy facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k08oJgt0oeEm",
        "outputId": "af5d4d02-87b7-4827-d45f-7b669b140e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b2\n",
            "📦 Loading model from: /content/checkpoint_epoch_2.pth\n",
            "✅ Model loaded successfully on cuda\n",
            "🚀 Starting deepfake detection pipeline...\n",
            "📹 Video: /content/Two_senior_politicians_202508101747_mlk3j.mp4\n",
            "🎬 Processing video: /content/Two_senior_politicians_202508101747_mlk3j.mp4\n",
            "📁 Output directory: /tmp/face_extraction_elt94xsx\n",
            "🔍 Extracting faces at 10.0 FPS...\n",
            "🔧 Post-processing sequences...\n",
            "✅ Face extraction complete!\n",
            "📊 Found 2 face tracks\n",
            "   Person 001: 96 frames\n",
            "   Person 002: 92 frames\n",
            "\n",
            "🔮 Running batch inference on 2 face tracks...\n",
            "\n",
            "👤 Processing Person 001:\n",
            "📸 Found 96 frames in /tmp/face_extraction_elt94xsx/person_001\n",
            "  🔄 Processing batch 1: frames 1 to 32 (32 frames)\n",
            "    📊 Batch 1 Result: FAKE (confidence: 0.591, fake_prob: 0.591)\n",
            "  🔄 Processing batch 2: frames 33 to 64 (32 frames)\n",
            "    📊 Batch 2 Result: FAKE (confidence: 0.925, fake_prob: 0.925)\n",
            "  🔄 Processing batch 3: frames 65 to 96 (32 frames)\n",
            "    📊 Batch 3 Result: FAKE (confidence: 0.868, fake_prob: 0.868)\n",
            "  📈 Person 001 Summary:\n",
            "     Total batches: 3\n",
            "     FAKE batches: 3\n",
            "     REAL batches: 0\n",
            "     Average confidence: 0.795\n",
            "     Average fake probability: 0.795\n",
            "     🎯 Person prediction: FAKE\n",
            "\n",
            "👤 Processing Person 002:\n",
            "📸 Found 95 frames in /tmp/face_extraction_elt94xsx/person_002\n",
            "  🔄 Processing batch 1: frames 1 to 32 (32 frames)\n",
            "    📊 Batch 1 Result: FAKE (confidence: 0.713, fake_prob: 0.713)\n",
            "  🔄 Processing batch 2: frames 33 to 64 (32 frames)\n",
            "    📊 Batch 2 Result: FAKE (confidence: 0.890, fake_prob: 0.890)\n",
            "  🔄 Processing batch 3: frames 65 to 95 (31 frames)\n",
            "    📊 Batch 3 Result: REAL (confidence: 0.668, fake_prob: 0.332)\n",
            "  📈 Person 002 Summary:\n",
            "     Total batches: 3\n",
            "     FAKE batches: 2\n",
            "     REAL batches: 1\n",
            "     Average confidence: 0.757\n",
            "     Average fake probability: 0.645\n",
            "     🎯 Person prediction: FAKE\n",
            "\n",
            "🧹 Cleaned up temporary directory\n",
            "\n",
            "📊 DETAILED BATCH RESULTS:\n",
            "============================================================\n",
            "\n",
            "PERSON_001:\n",
            "  Total Batches: 3\n",
            "  ----------------------------------------\n",
            "  Batch 1 (frames 1-32):\n",
            "    Prediction: FAKE\n",
            "    Confidence: 0.591\n",
            "    Real Probability: 0.409\n",
            "    Fake Probability: 0.591\n",
            "    Frames in batch: 32\n",
            "    Valid frames: 32\n",
            "\n",
            "  Batch 2 (frames 33-64):\n",
            "    Prediction: FAKE\n",
            "    Confidence: 0.925\n",
            "    Real Probability: 0.075\n",
            "    Fake Probability: 0.925\n",
            "    Frames in batch: 32\n",
            "    Valid frames: 32\n",
            "\n",
            "  Batch 3 (frames 65-96):\n",
            "    Prediction: FAKE\n",
            "    Confidence: 0.868\n",
            "    Real Probability: 0.132\n",
            "    Fake Probability: 0.868\n",
            "    Frames in batch: 32\n",
            "    Valid frames: 32\n",
            "\n",
            "  🎯 person_001 SUMMARY:\n",
            "    FAKE batches: 3/3\n",
            "    REAL batches: 0/3\n",
            "    Average confidence: 0.795\n",
            "    Overall person prediction: FAKE\n",
            "\n",
            "\n",
            "PERSON_002:\n",
            "  Total Batches: 3\n",
            "  ----------------------------------------\n",
            "  Batch 1 (frames 1-32):\n",
            "    Prediction: FAKE\n",
            "    Confidence: 0.713\n",
            "    Real Probability: 0.287\n",
            "    Fake Probability: 0.713\n",
            "    Frames in batch: 32\n",
            "    Valid frames: 32\n",
            "\n",
            "  Batch 2 (frames 33-64):\n",
            "    Prediction: FAKE\n",
            "    Confidence: 0.890\n",
            "    Real Probability: 0.110\n",
            "    Fake Probability: 0.890\n",
            "    Frames in batch: 32\n",
            "    Valid frames: 32\n",
            "\n",
            "  Batch 3 (frames 65-95):\n",
            "    Prediction: REAL\n",
            "    Confidence: 0.668\n",
            "    Real Probability: 0.668\n",
            "    Fake Probability: 0.332\n",
            "    Frames in batch: 31\n",
            "    Valid frames: 31\n",
            "\n",
            "  🎯 person_002 SUMMARY:\n",
            "    FAKE batches: 2/3\n",
            "    REAL batches: 1/3\n",
            "    Average confidence: 0.757\n",
            "    Overall person prediction: FAKE\n",
            "\n",
            "============================================================\n",
            "🎯 OVERALL VIDEO PREDICTION: FAKE\n",
            "   Average confidence: 0.776\n",
            "   Total batches analyzed: 6\n",
            "   FAKE batches: 5\n",
            "   REAL batches: 1\n",
            "   Fake ratio: 0.833\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from scipy.spatial.distance import cosine\n",
        "import shutil\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ==== Model Definition (Same as training script) ====\n",
        "class SpatialTemporalDeepfakeDetector(nn.Module):\n",
        "    def __init__(self, num_classes=2, max_frames=32, feature_dim=1408):\n",
        "        super().__init__()\n",
        "\n",
        "        # Spatial feature extractor\n",
        "        self.spatial_encoder = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        self.spatial_encoder._fc = nn.Identity()\n",
        "\n",
        "        # Spatial feature projection\n",
        "        self.spatial_projection = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Temporal modeling with LSTM + Transformer\n",
        "        self.temporal_lstm = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Temporal attention\n",
        "        self.temporal_attention = nn.MultiheadAttention(\n",
        "            embed_dim=512,  # bidirectional LSTM output\n",
        "            num_heads=8,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Positional encoding for temporal sequences\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(max_frames, 512))\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, valid_frames=None):  # x: (B, T, C, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Extract spatial features for each frame\n",
        "        x_reshaped = x.view(B * T, C, H, W)\n",
        "        spatial_features = self.spatial_encoder.extract_features(x_reshaped)  # (B*T, 1408, h, w)\n",
        "\n",
        "        # Project spatial features\n",
        "        spatial_features = self.spatial_projection(spatial_features)  # (B*T, 512)\n",
        "        spatial_features = spatial_features.view(B, T, -1)  # (B, T, 512)\n",
        "\n",
        "        # Add positional encoding\n",
        "        spatial_features = spatial_features + self.pos_encoding[:T].unsqueeze(0)\n",
        "\n",
        "        # Temporal modeling with LSTM\n",
        "        lstm_out, _ = self.temporal_lstm(spatial_features)  # (B, T, 512)\n",
        "\n",
        "        # Create attention mask for padded frames (optional)\n",
        "        if valid_frames is not None:\n",
        "            # Create mask for attention\n",
        "            mask = torch.zeros(B, T, dtype=torch.bool, device=x.device)\n",
        "            for i, vf in enumerate(valid_frames):\n",
        "                mask[i, :vf] = True\n",
        "            attn_mask = ~mask\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        # Temporal attention\n",
        "        attended_features, attention_weights = self.temporal_attention(\n",
        "            lstm_out, lstm_out, lstm_out, key_padding_mask=attn_mask\n",
        "        )  # (B, T, 512)\n",
        "\n",
        "        # Global temporal pooling (weighted by valid frames)\n",
        "        if valid_frames is not None:\n",
        "            # Weighted average based on valid frames\n",
        "            temporal_representation = []\n",
        "            for i, vf in enumerate(valid_frames):\n",
        "                temporal_representation.append(\n",
        "                    torch.mean(attended_features[i, :vf], dim=0)\n",
        "                )\n",
        "            temporal_representation = torch.stack(temporal_representation)\n",
        "        else:\n",
        "            temporal_representation = torch.mean(attended_features, dim=1)  # (B, 512)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(temporal_representation)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ==== Face Extraction Class ====\n",
        "class FaceExtractor:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.detector = MTCNN(keep_all=True, device=device)\n",
        "        self.embedder = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "        # Parameters\n",
        "        self.max_cosine_dist = 0.5\n",
        "        self.max_age = 30  # max frame age to retain a track\n",
        "        self.sequence_length = 32  # Match model's max_frames\n",
        "        self.max_gap_sec = 0.5\n",
        "        self.sample_interval_sec = 0.1\n",
        "\n",
        "    def extract_faces_from_video(self, video_path, output_dir=None):\n",
        "        \"\"\"Extract face sequences from a video\"\"\"\n",
        "        if output_dir is None:\n",
        "            output_dir = tempfile.mkdtemp(prefix='face_extraction_')\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"🎬 Processing video: {video_path}\")\n",
        "        print(f\"📁 Output directory: {output_dir}\")\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"❌ Failed to open {video_path}\")\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps == 0:\n",
        "            fps = 25.0\n",
        "        frame_interval = int(round(fps * self.sample_interval_sec))\n",
        "\n",
        "        # Reset state\n",
        "        tracks = {}\n",
        "        track_frames = {}\n",
        "        next_track_id = 1\n",
        "        frame_num = 0\n",
        "        processed_frame_count = 0\n",
        "\n",
        "        print(f\"🔍 Extracting faces at {1/self.sample_interval_sec:.1f} FPS...\")\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame_num += 1\n",
        "\n",
        "            if frame_num % frame_interval != 0:\n",
        "                continue\n",
        "\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # seconds\n",
        "\n",
        "            # Detect faces\n",
        "            boxes, probs = self.detector.detect(rgb)\n",
        "            if boxes is None:\n",
        "                continue\n",
        "\n",
        "            new_faces = []\n",
        "            for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
        "                if prob < 0.99:\n",
        "                    continue\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                margin = 10\n",
        "                x1 = max(0, x1 - margin)\n",
        "                y1 = max(0, y1 - margin)\n",
        "                x2 = min(frame.shape[1], x2 + margin)\n",
        "                y2 = min(frame.shape[0], y2 + margin)\n",
        "\n",
        "                face_crop = rgb[y1:y2, x1:x2]\n",
        "                if face_crop.size == 0 or face_crop.shape[0] < 60 or face_crop.shape[1] < 60:\n",
        "                    continue\n",
        "\n",
        "                resized = cv2.resize(face_crop, (160, 160))\n",
        "                face_tensor = torch.tensor(resized).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "                face_tensor = face_tensor.to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    embedding = self.embedder(face_tensor).cpu().numpy()[0]\n",
        "\n",
        "                new_faces.append((embedding, face_crop))\n",
        "\n",
        "            # Track assignment\n",
        "            assigned = []\n",
        "            for emb, crop in new_faces:\n",
        "                best_match = None\n",
        "                best_dist = float('inf')\n",
        "                for track_id, data in tracks.items():\n",
        "                    dist = cosine(emb, data['embedding'])\n",
        "                    if dist < best_dist and dist < self.max_cosine_dist:\n",
        "                        best_dist = dist\n",
        "                        best_match = track_id\n",
        "\n",
        "                if best_match is not None:\n",
        "                    tracks[best_match]['embedding'] = emb\n",
        "                    tracks[best_match]['last_frame'] = frame_num\n",
        "                    assigned.append((best_match, crop))\n",
        "                else:\n",
        "                    track_id = next_track_id\n",
        "                    next_track_id += 1\n",
        "                    tracks[track_id] = {'embedding': emb, 'last_frame': frame_num}\n",
        "                    track_frames[track_id] = []\n",
        "                    assigned.append((track_id, crop))\n",
        "\n",
        "            # Save frames\n",
        "            for track_id, crop in assigned:\n",
        "                save_path = os.path.join(output_dir, f'person_{track_id:03d}')\n",
        "                os.makedirs(save_path, exist_ok=True)\n",
        "                processed_frame_count += 1\n",
        "                filename = f'frame_{processed_frame_count:04d}_{frame_time:.2f}s.jpg'\n",
        "                filepath = os.path.join(save_path, filename)\n",
        "                cv2.imwrite(filepath, cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))\n",
        "                track_frames[track_id].append((frame_time, filepath))\n",
        "\n",
        "            # Clean old tracks\n",
        "            tracks = {k: v for k, v in tracks.items() if frame_num - v['last_frame'] <= self.max_age}\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Post-process sequences\n",
        "        self._post_process_sequences(track_frames)\n",
        "\n",
        "        print(f\"✅ Face extraction complete!\")\n",
        "        print(f\"📊 Found {len(track_frames)} face tracks\")\n",
        "        for track_id, frames in track_frames.items():\n",
        "            print(f\"   Person {track_id:03d}: {len(frames)} frames\")\n",
        "\n",
        "        return output_dir, track_frames\n",
        "\n",
        "    def _post_process_sequences(self, track_frames):\n",
        "        \"\"\"Pad sequences for RNN/LSTM processing\"\"\"\n",
        "        print(\"🔧 Post-processing sequences...\")\n",
        "\n",
        "        for track_id, frames in track_frames.items():\n",
        "            if not frames:\n",
        "                continue\n",
        "\n",
        "            # Sort original frames by timestamp\n",
        "            frames.sort(key=lambda x: x[0])\n",
        "            save_dir = os.path.dirname(frames[0][1])\n",
        "            valid_frames = [frames[0]]  # Start with first frame\n",
        "\n",
        "            # Fill time gaps with duplicates of previous frame\n",
        "            for i in range(1, len(frames)):\n",
        "                prev_time, prev_path = valid_frames[-1]\n",
        "                curr_time, curr_path = frames[i]\n",
        "                time_diff = curr_time - prev_time\n",
        "\n",
        "                if time_diff <= self.max_gap_sec:\n",
        "                    # Compute how many frames are missing based on sample interval\n",
        "                    n_missing = int(round(time_diff / self.sample_interval_sec)) - 1\n",
        "                    for m in range(n_missing):\n",
        "                        pad_time = prev_time + (m + 1) * self.sample_interval_sec\n",
        "                        pad_path = os.path.join(save_dir, f'frame_fill_tmp_{pad_time:.2f}s.jpg')\n",
        "                        shutil.copy(prev_path, pad_path)\n",
        "                        valid_frames.append((pad_time, pad_path))\n",
        "\n",
        "                    valid_frames.append((curr_time, curr_path))\n",
        "                else:\n",
        "                    break  # Stop this track if time jump too large\n",
        "\n",
        "            # Pad end of sequence if needed to reach required length\n",
        "            if len(valid_frames) < self.sequence_length:\n",
        "                last_time, last_path = valid_frames[-1]\n",
        "                for i in range(len(valid_frames), self.sequence_length):\n",
        "                    pad_time = last_time + (i - len(valid_frames) + 1) * self.sample_interval_sec\n",
        "                    pad_path = os.path.join(save_dir, f'frame_pad_tmp_{pad_time:.2f}s.jpg')\n",
        "                    shutil.copy(last_path, pad_path)\n",
        "                    valid_frames.append((pad_time, pad_path))\n",
        "\n",
        "            # Final sort and renaming to guarantee temporal order\n",
        "            valid_frames.sort(key=lambda x: x[0])\n",
        "            for i, (t, old_path) in enumerate(valid_frames):\n",
        "                new_filename = f'frame_{i+1:04d}_{t:.2f}s.jpg'\n",
        "                new_path = os.path.join(save_dir, new_filename)\n",
        "                if old_path != new_path:  # Only rename if different\n",
        "                    os.rename(old_path, new_path)\n",
        "\n",
        "# ==== Inference Class ====\n",
        "class DeepfakeInference:\n",
        "    def __init__(self, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.model = SpatialTemporalDeepfakeDetector().to(device)\n",
        "\n",
        "        # Load trained model\n",
        "        print(f\"📦 Loading model from: {model_path}\")\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            self.model.load_state_dict(checkpoint)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Image preprocessing\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(f\"✅ Model loaded successfully on {device}\")\n",
        "\n",
        "    def predict_sequence_batches(self, sequence_dir, max_frames=32, min_frames=16):\n",
        "        \"\"\"Predict on a face sequence directory using batches like training\"\"\"\n",
        "        try:\n",
        "            # Get all frame files\n",
        "            frame_files = []\n",
        "            for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "                import glob\n",
        "                frame_files.extend(glob.glob(os.path.join(sequence_dir, ext)))\n",
        "\n",
        "            frame_files = sorted(frame_files)\n",
        "\n",
        "            if len(frame_files) == 0:\n",
        "                print(f\"⚠️ No frames found in {sequence_dir}\")\n",
        "                return []\n",
        "\n",
        "            print(f\"📸 Found {len(frame_files)} frames in {sequence_dir}\")\n",
        "\n",
        "            # Create batches like in training\n",
        "            batches_results = []\n",
        "            total_frames = len(frame_files)\n",
        "\n",
        "            if total_frames < min_frames:\n",
        "                print(f\"⚠️ Skipping {sequence_dir}: only {total_frames} frames (< {min_frames})\")\n",
        "                return []\n",
        "\n",
        "            # Create non-overlapping batches (same as training)\n",
        "            stride = max_frames  # Non-overlapping batches\n",
        "            start_idx = 0\n",
        "            batch_num = 1\n",
        "\n",
        "            while start_idx < total_frames:\n",
        "                end_idx = min(start_idx + max_frames, total_frames)\n",
        "                batch_frame_files = frame_files[start_idx:end_idx]\n",
        "\n",
        "                # Only process batches that meet minimum frame requirement\n",
        "                if len(batch_frame_files) >= min_frames:\n",
        "                    print(f\"  🔄 Processing batch {batch_num}: frames {start_idx+1} to {end_idx} ({len(batch_frame_files)} frames)\")\n",
        "\n",
        "                    # Load and preprocess frames for this batch\n",
        "                    images = []\n",
        "                    successful_loads = 0\n",
        "\n",
        "                    for frame_path in batch_frame_files:\n",
        "                        try:\n",
        "                            img = Image.open(frame_path).convert('RGB')\n",
        "                            img = self.transform(img)\n",
        "                            images.append(img)\n",
        "                            successful_loads += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"⚠️ Error loading {frame_path}: {e}\")\n",
        "                            # Use black image as fallback\n",
        "                            fallback_img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "                            img = self.transform(fallback_img)\n",
        "                            images.append(img)\n",
        "\n",
        "                    if len(images) == 0:\n",
        "                        print(f\"⚠️ No valid frames loaded for batch {batch_num}\")\n",
        "                        start_idx += stride\n",
        "                        continue\n",
        "\n",
        "                    # Pad sequence to max_frames if needed (same as training)\n",
        "                    while len(images) < max_frames:\n",
        "                        images.append(images[-1].clone())  # Duplicate last frame\n",
        "\n",
        "                    # Convert to tensor and add batch dimension\n",
        "                    sequence = torch.stack(images).unsqueeze(0).to(self.device)  # (1, T, C, H, W)\n",
        "                    valid_frames = torch.tensor([successful_loads]).to(self.device)\n",
        "\n",
        "                    # Predict\n",
        "                    with torch.no_grad():\n",
        "                        outputs = self.model(sequence, valid_frames)\n",
        "                        probabilities = torch.softmax(outputs, dim=1)\n",
        "                        prediction = torch.argmax(outputs, dim=1)\n",
        "                        confidence = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "                    batch_result = {\n",
        "                        'batch_number': batch_num,\n",
        "                        'frame_range': f\"{start_idx+1}-{end_idx}\",\n",
        "                        'prediction': 'FAKE' if prediction.item() == 1 else 'REAL',\n",
        "                        'confidence': confidence.item(),\n",
        "                        'fake_probability': probabilities[0, 1].item(),\n",
        "                        'real_probability': probabilities[0, 0].item(),\n",
        "                        'frames_in_batch': len(batch_frame_files),\n",
        "                        'valid_frames': successful_loads,\n",
        "                        'start_frame': start_idx + 1,\n",
        "                        'end_frame': end_idx\n",
        "                    }\n",
        "\n",
        "                    batches_results.append(batch_result)\n",
        "\n",
        "                    # Print batch result immediately\n",
        "                    print(f\"    📊 Batch {batch_num} Result: {batch_result['prediction']} \"\n",
        "                          f\"(confidence: {batch_result['confidence']:.3f}, \"\n",
        "                          f\"fake_prob: {batch_result['fake_probability']:.3f})\")\n",
        "\n",
        "                    batch_num += 1\n",
        "\n",
        "                start_idx += stride\n",
        "\n",
        "                # If remaining frames are less than min_frames, break\n",
        "                if total_frames - start_idx < min_frames:\n",
        "                    break\n",
        "\n",
        "            return batches_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in predict_sequence_batches: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "\n",
        "# ==== Main Pipeline Class ====\n",
        "class DeepfakeDetectionPipeline:\n",
        "    def __init__(self, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.face_extractor = FaceExtractor(device)\n",
        "        self.inference = DeepfakeInference(model_path, device)\n",
        "\n",
        "    def process_video(self, video_path, cleanup_temp=True):\n",
        "        \"\"\"Complete pipeline: extract faces and run inference\"\"\"\n",
        "        print(f\"🚀 Starting deepfake detection pipeline...\")\n",
        "        print(f\"📹 Video: {video_path}\")\n",
        "\n",
        "        # Extract faces\n",
        "        temp_dir, track_frames = self.face_extractor.extract_faces_from_video(video_path)\n",
        "\n",
        "        if not track_frames:\n",
        "            print(\"❌ No faces found in video\")\n",
        "            return None\n",
        "\n",
        "        # Run inference on each face track\n",
        "        results = {}\n",
        "        print(f\"\\n🔮 Running batch inference on {len(track_frames)} face tracks...\")\n",
        "\n",
        "        for track_id, frames in track_frames.items():\n",
        "            person_dir = os.path.join(temp_dir, f'person_{track_id:03d}')\n",
        "\n",
        "            if os.path.exists(person_dir):\n",
        "                print(f\"\\n👤 Processing Person {track_id:03d}:\")\n",
        "                batch_results = self.inference.predict_sequence_batches(person_dir)\n",
        "\n",
        "                if batch_results:\n",
        "                    results[f'person_{track_id:03d}'] = {\n",
        "                        'batches': batch_results,\n",
        "                        'total_batches': len(batch_results)\n",
        "                    }\n",
        "\n",
        "                    # Print summary for this person\n",
        "                    fake_count = sum(1 for batch in batch_results if batch['prediction'] == 'FAKE')\n",
        "                    real_count = sum(1 for batch in batch_results if batch['prediction'] == 'REAL')\n",
        "                    avg_confidence = sum(batch['confidence'] for batch in batch_results) / len(batch_results)\n",
        "                    avg_fake_prob = sum(batch['fake_probability'] for batch in batch_results) / len(batch_results)\n",
        "\n",
        "                    print(f\"  📈 Person {track_id:03d} Summary:\")\n",
        "                    print(f\"     Total batches: {len(batch_results)}\")\n",
        "                    print(f\"     FAKE batches: {fake_count}\")\n",
        "                    print(f\"     REAL batches: {real_count}\")\n",
        "                    print(f\"     Average confidence: {avg_confidence:.3f}\")\n",
        "                    print(f\"     Average fake probability: {avg_fake_prob:.3f}\")\n",
        "\n",
        "                    # Determine overall prediction for this person\n",
        "                    person_prediction = \"FAKE\" if fake_count > real_count else \"REAL\"\n",
        "                    print(f\"     🎯 Person prediction: {person_prediction}\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"   ❌ Person {track_id:03d}: No valid batches processed\")\n",
        "\n",
        "        # Cleanup temporary directory\n",
        "        if cleanup_temp:\n",
        "            try:\n",
        "                shutil.rmtree(temp_dir)\n",
        "                print(f\"\\n🧹 Cleaned up temporary directory\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not cleanup temp directory: {e}\")\n",
        "        else:\n",
        "            print(f\"\\n📁 Temporary files kept in: {temp_dir}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_overall_prediction(self, results):\n",
        "        \"\"\"Get overall video prediction based on all face tracks and their batches\"\"\"\n",
        "        if not results:\n",
        "            return \"UNKNOWN\", 0.0\n",
        "\n",
        "        total_fake_batches = 0\n",
        "        total_real_batches = 0\n",
        "        total_confidence = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for person, person_data in results.items():\n",
        "            if 'batches' in person_data:\n",
        "                for batch in person_data['batches']:\n",
        "                    if batch['prediction'] == 'FAKE':\n",
        "                        total_fake_batches += 1\n",
        "                    else:\n",
        "                        total_real_batches += 1\n",
        "\n",
        "                    total_confidence += batch['confidence']\n",
        "                    total_batches += 1\n",
        "\n",
        "        if total_batches == 0:\n",
        "            return \"UNKNOWN\", 0.0\n",
        "\n",
        "        avg_confidence = total_confidence / total_batches\n",
        "\n",
        "        # If majority of batches are fake, classify as fake\n",
        "        if total_fake_batches > total_real_batches:\n",
        "            return \"FAKE\", avg_confidence\n",
        "        else:\n",
        "            return \"REAL\", avg_confidence\n",
        "\n",
        "# ==== Usage Example ====\n",
        "def main():\n",
        "    # Configuration\n",
        "    MODEL_PATH = '/content/checkpoint_epoch_2.pth'  # Path to your trained model\n",
        "    VIDEO_PATH = '/content/Two_senior_politicians_202508101747_mlk3j.mp4'  # Path to input video\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline\n",
        "        pipeline = DeepfakeDetectionPipeline(MODEL_PATH)\n",
        "\n",
        "        # Process video\n",
        "        results = pipeline.process_video(VIDEO_PATH, cleanup_temp=True)\n",
        "\n",
        "        if results:\n",
        "            print(f\"\\n📊 DETAILED BATCH RESULTS:\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            for person, person_data in results.items():\n",
        "                if 'batches' in person_data:\n",
        "                    print(f\"\\n{person.upper()}:\")\n",
        "                    print(f\"  Total Batches: {person_data['total_batches']}\")\n",
        "                    print(\"  \" + \"-\" * 40)\n",
        "\n",
        "                    for batch in person_data['batches']:\n",
        "                        print(f\"  Batch {batch['batch_number']} (frames {batch['frame_range']}):\")\n",
        "                        print(f\"    Prediction: {batch['prediction']}\")\n",
        "                        print(f\"    Confidence: {batch['confidence']:.3f}\")\n",
        "                        print(f\"    Real Probability: {batch['real_probability']:.3f}\")\n",
        "                        print(f\"    Fake Probability: {batch['fake_probability']:.3f}\")\n",
        "                        print(f\"    Frames in batch: {batch['frames_in_batch']}\")\n",
        "                        print(f\"    Valid frames: {batch['valid_frames']}\")\n",
        "                        print()\n",
        "\n",
        "                    # Person-level summary\n",
        "                    fake_batches = sum(1 for batch in person_data['batches'] if batch['prediction'] == 'FAKE')\n",
        "                    real_batches = sum(1 for batch in person_data['batches'] if batch['prediction'] == 'REAL')\n",
        "                    avg_conf = sum(batch['confidence'] for batch in person_data['batches']) / len(person_data['batches'])\n",
        "\n",
        "                    print(f\"  🎯 {person} SUMMARY:\")\n",
        "                    print(f\"    FAKE batches: {fake_batches}/{person_data['total_batches']}\")\n",
        "                    print(f\"    REAL batches: {real_batches}/{person_data['total_batches']}\")\n",
        "                    print(f\"    Average confidence: {avg_conf:.3f}\")\n",
        "                    person_pred = \"FAKE\" if fake_batches > real_batches else \"REAL\"\n",
        "                    print(f\"    Overall person prediction: {person_pred}\")\n",
        "                    print()\n",
        "\n",
        "            # Overall video prediction\n",
        "            overall_pred, overall_conf = pipeline.get_overall_prediction(results)\n",
        "\n",
        "            # Count total batches across all people\n",
        "            total_batches = sum(person_data['total_batches'] for person_data in results.values() if 'batches' in person_data)\n",
        "            total_fake = sum(sum(1 for batch in person_data['batches'] if batch['prediction'] == 'FAKE')\n",
        "                           for person_data in results.values() if 'batches' in person_data)\n",
        "            total_real = total_batches - total_fake\n",
        "\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"🎯 OVERALL VIDEO PREDICTION: {overall_pred}\")\n",
        "            print(f\"   Average confidence: {overall_conf:.3f}\")\n",
        "            print(f\"   Total batches analyzed: {total_batches}\")\n",
        "            print(f\"   FAKE batches: {total_fake}\")\n",
        "            print(f\"   REAL batches: {total_real}\")\n",
        "            print(f\"   Fake ratio: {total_fake/max(total_batches, 1):.3f}\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        else:\n",
        "            print(\"❌ No results obtained from video processing\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# ==== Standalone Function for Easy Use ====\n",
        "def detect_deepfake(video_path, model_path, cleanup_temp=True):\n",
        "    \"\"\"\n",
        "    Standalone function to detect deepfakes in a video\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video\n",
        "        model_path (str): Path to the trained model (.pth file)\n",
        "        cleanup_temp (bool): Whether to cleanup temporary files\n",
        "\n",
        "    Returns:\n",
        "        dict: Detection results for each face track found\n",
        "    \"\"\"\n",
        "    pipeline = DeepfakeDetectionPipeline(model_path)\n",
        "    return pipeline.process_video(video_path, cleanup_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Tg1tiXzzB3Ui",
        "outputId": "289e69f4-64ea-41d5-ad51-53722f672e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b2\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "max() iterable argument is empty",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3464494262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp/extracted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch predictions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2298315811.py\u001b[0m in \u001b[0;36minfer_video\u001b[0;34m(model, video_path, output_root, device)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrack_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrack_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mseq_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'person_{track_id:03d}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2298315811.py\u001b[0m in \u001b[0;36minfer_sequence\u001b[0;34m(model, seq_dir, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# Majority vote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mfinal_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() iterable argument is empty"
          ]
        }
      ],
      "source": [
        "# Initialize pipeline\n",
        "pipeline = DeepfakeDetectionPipeline('best_model.pth')\n",
        "\n",
        "# Process video\n",
        "results = pipeline.process_video('path/to/video.mp4')\n",
        "\n",
        "# Get overall prediction\n",
        "overall_pred, confidence = pipeline.get_overall_prediction(results)\n",
        "print(f\"Video is {overall_pred} with {confidence:.3f} confidence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQShMIWrCG-2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06a38ef815cc44a996de1ff144421474": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090a19958ad849dda60e05f4a3499132": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12a604cf454d49faa3a70d4b74f1d382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1c6486e04243460bbc462141ce0cf834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd7f319c8d44cadb0e3cd573f0b6578",
            "placeholder": "​",
            "style": "IPY_MODEL_71bacabea97744029240057928a4d958",
            "value": "Connecting..."
          }
        },
        "283c175e4029448883b74730b0c4cac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29026a3d238a4487b50e6f50b7dbe466": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3b2c625b67fe4acb913cd0eea3c1b3ab",
            "placeholder": "​",
            "style": "IPY_MODEL_283c175e4029448883b74730b0c4cac3",
            "value": "binshilin"
          }
        },
        "2fd7f319c8d44cadb0e3cd573f0b6578": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e01af87ffe4ec4b3eccc104868a252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b8904ddf76894d83b950dbe58bd4e57e",
            "style": "IPY_MODEL_12a604cf454d49faa3a70d4b74f1d382",
            "tooltip": ""
          }
        },
        "3b2c625b67fe4acb913cd0eea3c1b3ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543c9b42a79e44bca577fd9ed81fac14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669acd1f0a9f4b9d8602440cbba8a277": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8bd986bc9f4d148ee753c3339c51f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669acd1f0a9f4b9d8602440cbba8a277",
            "placeholder": "​",
            "style": "IPY_MODEL_941a0abcfeec42efaa006b23c8a20838",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "71bacabea97744029240057928a4d958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "941a0abcfeec42efaa006b23c8a20838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3235b114c564c6d9ead4a983a346e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9562536a93a415e9bd3778c427fe940",
            "placeholder": "​",
            "style": "IPY_MODEL_bff307168ce74a63a2bd8be5309d642d",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "aceeeb4144bd4d9fa213fdb4590192a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7608b73efca4de6b3025d7d20a70173": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_543c9b42a79e44bca577fd9ed81fac14",
            "placeholder": "​",
            "style": "IPY_MODEL_aceeeb4144bd4d9fa213fdb4590192a7",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "b8904ddf76894d83b950dbe58bd4e57e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9562536a93a415e9bd3778c427fe940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff307168ce74a63a2bd8be5309d642d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e23d468e2cb14a73a97fa77af2bd175f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7608b73efca4de6b3025d7d20a70173"
            ],
            "layout": "IPY_MODEL_fa45806238ea494eb9f33f8c2e7f467f"
          }
        },
        "fa45806238ea494eb9f33f8c2e7f467f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fce0bc7a6eba4081a3695e4fabc2174d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_06a38ef815cc44a996de1ff144421474",
            "placeholder": "​",
            "style": "IPY_MODEL_090a19958ad849dda60e05f4a3499132",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
