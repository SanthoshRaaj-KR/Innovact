{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ef1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ===== 1. Load dataset =====\n",
    "dataset = load_dataset(\"project-droid/DroidCollection\")\n",
    "\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "dev_dataset   = dataset[\"dev\"]\n",
    "test_dataset  = dataset[\"test\"]\n",
    "\n",
    "# ===== 3. Remap string labels to 0/1 =====\n",
    "def remap_labels(example):\n",
    "    example[\"Label\"] = 0 if example[\"Label\"] == \"HUMAN_GENERATED\" else 1\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(remap_labels)\n",
    "dev_dataset   = dev_dataset.map(remap_labels)\n",
    "test_dataset  = test_dataset.map(remap_labels)\n",
    "\n",
    "# ===== 4. Load tokenizer =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "\n",
    "# ===== 5. Tokenization function =====\n",
    "def tokenize_fn(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"Code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"Label\"] = examples[\"Label\"]  # keep label\n",
    "    return tokens\n",
    "\n",
    "# ===== 6. Tokenize datasets =====\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "dev_dataset   = dev_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset  = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"Label\", \"labels\")\n",
    "dev_dataset   = dev_dataset.rename_column(\"Label\", \"labels\")\n",
    "test_dataset  = test_dataset.rename_column(\"Label\", \"labels\")\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_dataset.set_format(type=\"torch\", columns=columns)\n",
    "dev_dataset.set_format(type=\"torch\", columns=columns)\n",
    "test_dataset.set_format(type=\"torch\", columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda59336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# -------------------------------\n",
    "# Model Definition\n",
    "# -------------------------------\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "TEXT_EMBEDDING_DIM = 1024  # ModernBERT-large hidden size\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Handle both (preds, labels) tuple and EvalPrediction object\n",
    "    if isinstance(eval_pred, tuple):\n",
    "        preds, labels = eval_pred\n",
    "    else:\n",
    "        preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # If predictions is (logits, ...), take logits\n",
    "    if isinstance(preds, (tuple, list)):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Convert logits -> class ids\n",
    "    if preds.ndim > 1:\n",
    "        y_pred = np.argmax(preds, axis=-1)\n",
    "    else:\n",
    "        # fallback for odd shapes (binary/logistic)\n",
    "        y_pred = (preds > 0).astype(int)\n",
    "\n",
    "    y_true = labels\n",
    "\n",
    "    # Flatten and ignore label -100 if present\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    mask = (y_true != -100)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    # Derive num_classes from data (fallback to 2)\n",
    "    num_classes = int(max(y_true.max(initial=0), y_pred.max(initial=0)) + 1) if y_true.size else 2\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if 0 <= t < num_classes and 0 <= p < num_classes:\n",
    "            cm[t, p] += 1\n",
    "\n",
    "    total = cm.sum()\n",
    "    acc = (np.trace(cm) / total) if total > 0 else 0.0\n",
    "\n",
    "    tp = np.diag(cm).astype(float)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "\n",
    "    precision = np.divide(tp, tp + fp, out=np.zeros_like(tp), where=(tp + fp) != 0)\n",
    "    recall    = np.divide(tp, tp + fn, out=np.zeros_like(tp), where=(tp + fn) != 0)\n",
    "    f1        = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(tp), where=(precision + recall) != 0)\n",
    "\n",
    "    macro_f1 = f1.mean() if num_classes > 0 else 0.0\n",
    "    weights = cm.sum(axis=1) / total if total > 0 else np.zeros(num_classes)\n",
    "    weighted_f1 = float((f1 * weights).sum()) if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": float(macro_f1),\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "    }\n",
    "\n",
    "class TLModel(nn.Module):\n",
    "    def __init__(self, text_encoder, projection_dim=128, num_classes=NUM_CLASSES, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.num_classes = num_classes\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        # Project embeddings down\n",
    "        self.text_projection = nn.Linear(TEXT_EMBEDDING_DIM, projection_dim)\n",
    "        self.classifier = nn.Linear(projection_dim, num_classes)\n",
    "\n",
    "        # Loss functions\n",
    "        self.ce_loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if self.class_weights is not None else None\n",
    "        )\n",
    "        self.triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "    # Encode text\n",
    "        hidden = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        embeddings = hidden.mean(dim=1)  # mean pooling\n",
    "        projected = F.relu(self.text_projection(embeddings))\n",
    "        logits = self.classifier(projected)\n",
    "\n",
    "        loss, ce_loss, triplet_loss = None, None, None\n",
    "        if labels is not None:\n",
    "            # Cross-entropy loss\n",
    "            ce_loss = self.ce_loss_fn(logits, labels)\n",
    "\n",
    "            # Triplet loss mining\n",
    "            anchors, positives, negatives = [], [], []\n",
    "            for i in range(len(labels)):\n",
    "                anchor = projected[i]\n",
    "                label = labels[i].item()\n",
    "\n",
    "                pos_mask = (labels == label).nonzero(as_tuple=True)[0]\n",
    "                pos_mask = pos_mask[pos_mask != i]\n",
    "                if len(pos_mask) == 0:\n",
    "                    continue\n",
    "                pos_dists = torch.norm(projected[pos_mask] - anchor.unsqueeze(0), dim=1)\n",
    "                hardest_pos_idx = pos_mask[pos_dists.argmax()]\n",
    "\n",
    "                neg_mask = (labels != label).nonzero(as_tuple=True)[0]\n",
    "                if len(neg_mask) == 0:\n",
    "                    continue\n",
    "                neg_dists = torch.norm(projected[neg_mask] - anchor.unsqueeze(0), dim=1)\n",
    "                hardest_neg_idx = neg_mask[neg_dists.argmin()]\n",
    "\n",
    "                anchors.append(anchor)\n",
    "                positives.append(projected[hardest_pos_idx])\n",
    "                negatives.append(projected[hardest_neg_idx])\n",
    "\n",
    "            if anchors:\n",
    "                anchor_tensor = torch.stack(anchors)\n",
    "                positive_tensor = torch.stack(positives)\n",
    "                negative_tensor = torch.stack(negatives)\n",
    "                triplet_loss = self.triplet_loss_fn(anchor_tensor, positive_tensor, negative_tensor)\n",
    "\n",
    "            if triplet_loss is not None:\n",
    "                loss = ce_loss + 0.1 * triplet_loss\n",
    "            else:\n",
    "                loss = ce_loss\n",
    "\n",
    "        # âœ… Always return a SequenceClassifierOutput\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=projected,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Trainer requires at least {\"loss\", \"logits\"}\n",
    "        output = {\"logits\": logits, \"hidden_states\": projected}\n",
    "        if loss is not None:\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "text_encoder = AutoModel.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "\n",
    "model = TLModel(text_encoder=text_encoder)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./droiddetect_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=true\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./droiddetect_model/final\")\n",
    "torch.save(model.state_dict(), \"code_plagiarism.bin\")\n",
    "\n",
    "print(\"âœ… Training complete. Best model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
